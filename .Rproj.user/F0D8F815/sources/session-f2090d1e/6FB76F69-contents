---
title: "scRNA-Seq Quality Control and Sample Integration"
output: 
  html_document: 
    toc: yes
    toc_float: TRUE
    toc_depth: 2
    theme: cerulean
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = F, 
                      warning = F,
                      message = F,
                      dpi = 600,
                      fig.width = 7,
                      cache = T,
                      dev = "png")

```

# **Load Libraries and Plot Themes**

```{r, echo=F, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Loads all libraries, color schemes, and plot themes

source("1_Scripts/0_universal_lib_import.R",
       local = knitr::knit_global())


```


# **Parameters Specific to Individual Data Set (Supply with each .rmd file [e.g. QC, Clustering, DGE])**

```{r, echo=F, fig.align= "center", results=T, message=T, warning=T, eval=T}

#### NKX2.1 KO analysis in LSAE ####

# Study Design
## Large and small airway explants with either normal NKX2.1 gene expression or NKX2.1 KO at 2 timepoints (D7 and D28); testing KO efficiency and effect on cells before full differentiation
## N=3 per group, except in LAE control group ("D42S_LAE_NG_D7" failed during sequencing with too few barcodes)
## Total N=24 - 1 (23 Total samples)

# Project-specific parameters

## Sample file directory (contains cellranger outputs)

data.path <- paste("2_Data",
                   sep = "")


## Specific metadata columns

data.list.spec <- data.frame(code = unlist(lapply(strsplit(data.list.files,
                                                      "_",
                                                      fixed = T),
                                             "[",
                                             1)
                        ),
                        nkx = ifelse(grepl("NG",
                                           data.list.files),
                                     "NG",
                                     "KO"),
                        airway = ifelse(grepl("LAE",
                                              data.list.files),
                                        "LAE",
                                        "SAE"),
                        Time = ifelse(grepl("D28",
                                            data.list.files),
                                      "D28",
                                      "D7"))

names(data.list.spec) <- c("Code","Knockout",
                          "Airway","Time")


## Set column type to factor

data.list.spec[,c("Code","Knockout",
                  "Airway","Time")] <- data.frame(Code = factor(data.list$Code,
                         levels = c("D24S","D31S",
                                    "D42S","D71Q")),
                   Knockout = factor(data.list$Knockout,
                                     c("NG",
                                       "KO")),
                   Airway = factor(data.list$Airway,
                                   c("LAE",
                                     "SAE")),
                   Time = factor(data.list$Time,
                                 c("D7",
                                   "D28"))
                   )

## How many batches should be included in data integration, and what are the indices of each sample?

data.files.integrated <- lapply(data.list.norm,
                                function(x) x[[1]])

remove(data.list.norm)

num.batch <- round(length(data.files.integrated)/6,
                   digits = 0)

num.samp <- seq.int(1,length(data.files.integrated),1)


### Divide into quantiles and create individual subsets

q.subs <- list(q1.samp = num.samp[num.samp < 
                      quantile(num.samp,0.25)],

q2.samp = num.samp[num.samp < 
                      quantile(num.samp,0.51) &
                      num.samp > 
                      quantile(num.samp,0.25)],

q3.samp = num.samp[num.samp < 
                      quantile(num.samp,0.8) &
                      num.samp > 
                      quantile(num.samp,0.51)],

q4.samp = num.samp[num.samp > 
                      quantile(num.samp,0.8)])




## remove compiled list

remove(data.files.integrated)


```






# **Load raw data and complete contamination removal, filtering, and normalization**

## Recognizes the raw and filtered features within a sample folder
### Each sample should have its own unique folder "sample_name/" and there should be subfolders for "filtered_features/" and "raw_features/" to run the load10X() function for contaminate removal

# **Quality Control and Normalization**

## Removing low-quality or doublet cells by assessing number of genes per cell, number of molecules detected in each cell (high correlation with unique number of genes), and percentage of reads that map to the mitochondrial genome (high number indicates a dying cell and therefore should be excluded from downstream analysis).


# **1.1 Load Datasets**

```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Load Data Files

source("1_Scripts/sc_11QC_dataimport.R",
       local = knitr::knit_global())


```





# **1.2 Ambient RNA Removal**

```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Estimate contamination fraction (output is adjusted counts after ambient RNA removal per cell)

source("1_Scripts/sc_12QC_contrem.R",
       local = knitr::knit_global())

## Contamination fraction plot (select variables from data.list for point fill and color)

p.rho.fun(data.list,
          "Airway",
          "Knockout",
          "3_Analysis/1_QC/1_est_contamination.png")

## Plot cell and cluster number

p.fun1(data.list,
       "Cells",
       "Airway",
       "Knockout",
       "3_Analysis/1_QC/1_cell_number.png")


p.fun1(data.list,
       "Clusters",
       "Airway",
       "Knockout",
       "3_Analysis/1_QC/1_clus_number.png")


```





# **1.3 Doublet Cell Removal**

```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Remove doublet cells

source("1_Scripts/sc_13QC_doubrem.R",
       local = knitr::knit_global())

## save metadata for use in creating Seurat object

write.xlsx(data.list,
           "3_Analysis/1_QC/1_file_metadata.xlsx",
           overwrite = T)


```





# **1.4 Create Seurat Objects for Each Sample**

```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

## Source script to create each object and concatenate into list

source("1_Scripts/sc_14QC_seurcreate.R",
       local = knitr::knit_global())


```





# **1.5 Subset Seurat Objects Based on Counts and Percent of Reads Mapped to Mitochondrial Genome**

```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Subset Seurat objects

source("1_Scripts/sc_15QC_seursubset.R",
       local = knitr::knit_global())

## Second argument specifies filepath for saving .rds files for pre- and post-normalized QC plots

data.list.ser <- lapply(data.list.ser,
                        function(x) d.qc.fun(x,
                                             "3_Analysis/1_QC/"))


```






# **1.6 Log Normalize Each Seurat Object Prior to Integration**


```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Perform log normalization
      
source("1_Scripts/sc_16QC_normal.R",
       local = knitr::knit_global())

data.list.norm <- lapply(data.list.ser,
                         function (x) d.norm.fun(x,
                                                 "3_Analysis/1_QC/"))


# Save pre- and post-normalized lists as .rds (uncomment to run code and save; takes ~5-10 minutes per file to complete)

## Pre

# saveRDS(data.list.ser,
#         file = "3_Analysis/1_QC/1_prenorm_data.rds")

## Post

# saveRDS(data.list.norm,
#         file = "3_Analysis/1_QC/1_posnorm_data.rds")

# Save each sample as individual rds file for cell prediction

data.list.norm <- readRDS("3_Analysis/1_QC/1_posnorm_data.rds")

data.files.azi <- lapply(data.list.norm,
                         function(x) x[[1]])

for (i in 1:length(data.files.azi)) {
  
  saveRDS(data.files.azi[[i]],
          file = paste("3_Analysis/2_Integration/1_Individual_for_Azimuth/1_Individual_sample_",
                       paste(i),
                       ".rds",
                       sep = "")
          )
  
}





```





# **1.7 Integrate Samples**
## **Memory Intensive Step: Takes several hours [based on Windows PC with 128Gb RAM]**
## **Iteratively integrate samples to reduce memory usage [recommend < 6 samples per batch]**

```{r, echo=T, fig.align= "center", results=T, message=T, warning=T, eval=T}

# Remove pre-normalized Seurat objects

remove(data.list.ser)

# Find integration anchors (in parallel)

## Split into multiple sets and integrate separately to conserve computational resources

data.list.norm <- readRDS("3_Analysis/1_QC/1_posnorm_data.rds")

data.files.integrated <- lapply(data.list.norm,
                                function(x) x[[1]])

remove(data.list.norm)

## Subset

data.files.integrated.list <- vector("list",
                                     length = num.batch)

for (i in 1:length(data.files.integrated.list)) {
  
  data.files.integrated.list[[i]] <- data.files.integrated[q.subs[[i]]]
  
}


## remove compiled list

remove(data.files.integrated)


## Run integration function for each sample subset

source("1_Scripts/sc_17QC_integrate.R",
       local = knitr::knit_global())

### First Iteration

data.files.merged.list1 <- lapply(data.files.integrated.list,
                                  function(x) d.anchor.int.fun(x,
                                                               5000))

remove(data.files.integrated.list)


### Verify that all samples are present in integrated subsets

unique(levels(c(data.files.merged.list1[[1]]@active.ident,
                data.files.merged.list1[[2]]@active.ident,
                data.files.merged.list1[[3]]@active.ident,
                data.files.merged.list1[[4]]@active.ident)))



# repeat anchor/integration function for subsets 1-2 then 3-4

data.files.merged.list2 <- list(list(data.files.merged.list1[[1]],
                data.files.merged.list1[[2]]),
                list(data.files.merged.list1[[3]],
                data.files.merged.list1[[4]]))

data.files.merged.list2 <- lapply(data.files.merged.list2,
                                  function(x) d.anchor.int.fun(x,
                                                               7500))

remove(data.files.merged.list1)


### Verify that all samples are present in integrated subsets

unique(levels(c(data.files.merged.list2[[1]]@active.ident,
                data.files.merged.list2[[2]]@active.ident)))



# Last iteration: find integration anchors/integrate pairs 1 & 2

data.files.merged.all <- d.anchor.int.fun(data.files.merged.list2,
                                          13000)





```













